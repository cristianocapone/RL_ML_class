ğŸœ Ant-v4 Reinforcement Learning with Custom Recurrent Network

This repository contains a reinforcement learning (RL) agent that learns to walk in the MuJoCo Ant-v4 environment using a custom recurrent neural network architecture with biologically-inspired dynamics.

https://gym.openai.com/envs/Ant-v4/

ğŸš€ Features

âœ… Uses Ant-v4 MuJoCo environment from OpenAI Gym
âœ… Custom RL architecture with:
Leaky integrator dynamics
Reinforcement-driven Hebbian plasticity
Actor-Critic-like updates (with value prediction)
âœ… Periodic rendering and video saving
âœ… Automatic training progress plots
âœ… Save/load model with pickle
âœ… Highly customizable network and training parameters
ğŸ§  Architecture

The network (net) is a custom time-dependent recurrent model where:

Neurons have leaky membrane potentials
Synaptic traces and output plasticity evolve over time
Policy is learned through continuous interaction with the environment and internal credit assignment
ğŸ Getting Started

ğŸ”§ Requirements
Install MuJoCo and Gym:

pip install gym[mujoco] matplotlib numpy tqdm
MuJoCo setup (if not already done):

pip install mujoco
ğŸ“‚ Run Training
# Inside your notebook or script
!git clone https://github.com/yourusername/ant-v4-recurrent-rl.git
%cd ant-v4-recurrent-rl
!python train.py
(Make sure train.py contains the code you posted, or split it into modular files)

ğŸ“Š Training Outputs

rewards_ant_hier_transfer.png â€” periodic training plots
AntMujocoEnv-v4*.mp4 â€” rendered episodes saved as videos
transfer_new.pickle â€” trained model weights
trajectory_0.npy â€” reward trajectory data
ğŸ“ Parameters

You can tune parameters like:

episode_duration: episode length
act_variance: action exploration noise
alpha_pg, alpha_rout: learning rates
out_dim: output action dimension (default = 8 for Ant)
N, I, O, T: network size
ğŸ“¦ File Structure

ant-v4-recurrent-rl/
â”œâ”€â”€ train.py                # Main training loop
â”œâ”€â”€ net.py                  # Custom network definition (assumed)
â”œâ”€â”€ transfer_new.pickle     # Trained model (saved periodically)
â”œâ”€â”€ trajectory_0.npy        # Reward trajectory
â”œâ”€â”€ rewards_ant_hier_transfer.png
â””â”€â”€ AntMujocoEnv-v4*.mp4    # Rendered video episodes

ğŸ’¾ Model Persistence

To resume training or transfer learning:

with open('transfer.pickle', 'rb') as f:
    network = pickle.load(f)
ğŸ“½ï¸ Example Rendering

Set render_every = 100 to enable periodic rendering and save an .mp4 of the ant agent walking.

ğŸ¤– Future Plans

Add Gym wrapper to support other MuJoCo environments
Modularize network into reusable class
Support PyTorch version
Upload pretrained weights
ğŸ“œ License

MIT License
